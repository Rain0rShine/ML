{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0d59f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7f60508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\emma\\anaconda3\\lib\\site-packages (2.2.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: lxml in c:\\users\\emma\\anaconda3\\lib\\site-packages (5.2.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\emma\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\emma\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\emma\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\emma\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\emma\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "pip install pandas lxml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d07bfc64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 43533711 translation units\n",
      "                                             english  \\\n",
      "0               Previously on The Hot Zone: Anthrax.   \n",
      "1  Director Mueller just assigned us a major case...   \n",
      "2  Investigation''s  officially been dubbed Ameri...   \n",
      "3  Whoever sent these  letters got their Anthrax ...   \n",
      "4  We wouldn''t be here if we didn''t have eviden...   \n",
      "\n",
      "                                             swedish  \n",
      "0                              I tidigare avsnitt...  \n",
      "1      Byråchef Mueller gav oss just ett stort fall.  \n",
      "2            Utredningen har fått namnet Amerithrax.  \n",
      "3  Brevskickaren fick sin mjältbrand från ett ame...  \n",
      "4  Vi hade inte varit här om inte bevisen pekat p...  \n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import pandas as pd\n",
    "from lxml import etree\n",
    "\n",
    "def process_tmx_chunked(file_path, chunk_size=10000):\n",
    "    # Create an empty list to store DataFrames\n",
    "    chunks = []\n",
    "    \n",
    "    # Initialize variables for chunk processing\n",
    "    current_chunk = []\n",
    "    processed_count = 0\n",
    "    \n",
    "    # Use iterparse for memory-efficient parsing\n",
    "    context = etree.iterparse(\n",
    "        gzip.open(file_path), \n",
    "        events=('end',), \n",
    "        tag='tu'\n",
    "    )\n",
    "    \n",
    "    for event, elem in context:\n",
    "        try:\n",
    "            variants = {}\n",
    "            for tuv in elem.findall('tuv'):\n",
    "                lang = tuv.get('{http://www.w3.org/XML/1998/namespace}lang')\n",
    "                seg = tuv.find('seg')\n",
    "                variants[lang] = seg.text if seg is not None else ''\n",
    "            \n",
    "            if 'en' in variants and 'sv' in variants:\n",
    "                current_chunk.append({\n",
    "                    'english': variants['en'],\n",
    "                    'swedish': variants['sv']\n",
    "                })\n",
    "                processed_count += 1\n",
    "                \n",
    "                # When chunk is full, convert to DataFrame and store\n",
    "                if len(current_chunk) >= chunk_size:\n",
    "                    chunks.append(pd.DataFrame(current_chunk))\n",
    "                    current_chunk = []\n",
    "        finally:\n",
    "            # Clear memory after processing each element\n",
    "            elem.clear()\n",
    "            while elem.getprevious() is not None:\n",
    "                del elem.getparent()[0]\n",
    "    \n",
    "    # Add any remaining items in the last chunk\n",
    "    if current_chunk:\n",
    "        chunks.append(pd.DataFrame(current_chunk))\n",
    "    \n",
    "    # Combine all chunks into a single DataFrame\n",
    "    final_df = pd.concat(chunks, ignore_index=True)\n",
    "    \n",
    "    print(f\"Processed {processed_count} translation units\")\n",
    "    return final_df\n",
    "\n",
    "# Usage:\n",
    "df = process_tmx_chunked('en-sv.tmx.gz')\n",
    "print(df.head())  # Now this will work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9774b226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 43533711 entries, 0 to 43533710\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Dtype \n",
      "---  ------   ----- \n",
      " 0   english  object\n",
      " 1   swedish  object\n",
      "dtypes: object(2)\n",
      "memory usage: 664.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f061ed70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>swedish</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Previously on The Hot Zone: Anthrax.</td>\n",
       "      <td>I tidigare avsnitt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Director Mueller just assigned us a major case...</td>\n",
       "      <td>Byråchef Mueller gav oss just ett stort fall.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Investigation''s  officially been dubbed Ameri...</td>\n",
       "      <td>Utredningen har fått namnet Amerithrax.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Whoever sent these  letters got their Anthrax ...</td>\n",
       "      <td>Brevskickaren fick sin mjältbrand från ett ame...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We wouldn''t be here if we didn''t have eviden...</td>\n",
       "      <td>Vi hade inte varit här om inte bevisen pekat p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             english  \\\n",
       "0               Previously on The Hot Zone: Anthrax.   \n",
       "1  Director Mueller just assigned us a major case...   \n",
       "2  Investigation''s  officially been dubbed Ameri...   \n",
       "3  Whoever sent these  letters got their Anthrax ...   \n",
       "4  We wouldn''t be here if we didn''t have eviden...   \n",
       "\n",
       "                                             swedish  \n",
       "0                              I tidigare avsnitt...  \n",
       "1      Byråchef Mueller gav oss just ett stort fall.  \n",
       "2            Utredningen har fått namnet Amerithrax.  \n",
       "3  Brevskickaren fick sin mjältbrand från ett ame...  \n",
       "4  Vi hade inte varit här om inte bevisen pekat p...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32a973e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>swedish</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43533706</th>\n",
       "      <td>You are already almost 15 minutes late. Oh, my...</td>\n",
       "      <td>-Gå nu, du är nästan en kvart sen.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43533707</th>\n",
       "      <td>By the powers vested in me by the state of Sou...</td>\n",
       "      <td>I kraft av mitt ämbete i staten South Carolina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43533708</th>\n",
       "      <td>Who invited you? - I'm-- - Beat it.</td>\n",
       "      <td>Vem bjöd in dig?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43533709</th>\n",
       "      <td>Okay.</td>\n",
       "      <td>! Stick!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43533710</th>\n",
       "      <td>You may now kiss your bride.</td>\n",
       "      <td>Ni kan kyssa er brud.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    english  \\\n",
       "43533706  You are already almost 15 minutes late. Oh, my...   \n",
       "43533707  By the powers vested in me by the state of Sou...   \n",
       "43533708                Who invited you? - I'm-- - Beat it.   \n",
       "43533709                                              Okay.   \n",
       "43533710                       You may now kiss your bride.   \n",
       "\n",
       "                                                    swedish  \n",
       "43533706                 -Gå nu, du är nästan en kvart sen.  \n",
       "43533707  I kraft av mitt ämbete i staten South Carolina...  \n",
       "43533708                                   Vem bjöd in dig?  \n",
       "43533709                                           ! Stick!  \n",
       "43533710                              Ni kan kyssa er brud.  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56531587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "df.to_csv('en-sv_translations.csv', index=False)\n",
    "\n",
    "# Later load it with:\n",
    "df = pd.read_csv('en-sv_translations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c110d11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 43533711 entries, 0 to 43533710\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Dtype \n",
      "---  ------   ----- \n",
      " 0   english  object\n",
      " 1   swedish  object\n",
      "dtypes: object(2)\n",
      "memory usage: 664.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7d7ffd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>swedish</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Previously on The Hot Zone: Anthrax.</td>\n",
       "      <td>I tidigare avsnitt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Director Mueller just assigned us a major case...</td>\n",
       "      <td>Byråchef Mueller gav oss just ett stort fall.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Investigation''s  officially been dubbed Ameri...</td>\n",
       "      <td>Utredningen har fått namnet Amerithrax.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Whoever sent these  letters got their Anthrax ...</td>\n",
       "      <td>Brevskickaren fick sin mjältbrand från ett ame...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We wouldn''t be here if we didn''t have eviden...</td>\n",
       "      <td>Vi hade inte varit här om inte bevisen pekat p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43533706</th>\n",
       "      <td>You are already almost 15 minutes late. Oh, my...</td>\n",
       "      <td>-Gå nu, du är nästan en kvart sen.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43533707</th>\n",
       "      <td>By the powers vested in me by the state of Sou...</td>\n",
       "      <td>I kraft av mitt ämbete i staten South Carolina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43533708</th>\n",
       "      <td>Who invited you? - I'm-- - Beat it.</td>\n",
       "      <td>Vem bjöd in dig?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43533709</th>\n",
       "      <td>Okay.</td>\n",
       "      <td>! Stick!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43533710</th>\n",
       "      <td>You may now kiss your bride.</td>\n",
       "      <td>Ni kan kyssa er brud.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43533688 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    english  \\\n",
       "0                      Previously on The Hot Zone: Anthrax.   \n",
       "1         Director Mueller just assigned us a major case...   \n",
       "2         Investigation''s  officially been dubbed Ameri...   \n",
       "3         Whoever sent these  letters got their Anthrax ...   \n",
       "4         We wouldn''t be here if we didn''t have eviden...   \n",
       "...                                                     ...   \n",
       "43533706  You are already almost 15 minutes late. Oh, my...   \n",
       "43533707  By the powers vested in me by the state of Sou...   \n",
       "43533708                Who invited you? - I'm-- - Beat it.   \n",
       "43533709                                              Okay.   \n",
       "43533710                       You may now kiss your bride.   \n",
       "\n",
       "                                                    swedish  \n",
       "0                                     I tidigare avsnitt...  \n",
       "1             Byråchef Mueller gav oss just ett stort fall.  \n",
       "2                   Utredningen har fått namnet Amerithrax.  \n",
       "3         Brevskickaren fick sin mjältbrand från ett ame...  \n",
       "4         Vi hade inte varit här om inte bevisen pekat p...  \n",
       "...                                                     ...  \n",
       "43533706                 -Gå nu, du är nästan en kvart sen.  \n",
       "43533707  I kraft av mitt ämbete i staten South Carolina...  \n",
       "43533708                                   Vem bjöd in dig?  \n",
       "43533709                                           ! Stick!  \n",
       "43533710                              Ni kan kyssa er brud.  \n",
       "\n",
       "[43533688 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b496dac8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tokenizer\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequence\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pad_sequences\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dae89a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install keras-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1e3d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Clean the data first\n",
    "def clean_data(df):\n",
    "    # Remove rows with None/NaN in either column\n",
    "    df = df.dropna(subset=['swedish', 'english'])\n",
    "    \n",
    "    # Convert to string and strip whitespace\n",
    "    df['swedish'] = df['swedish'].astype(str).str.strip()\n",
    "    df['english'] = df['english'].astype(str).str.strip()\n",
    "    \n",
    "    # Remove empty strings\n",
    "    df = df[(df['swedish'] != '') & (df['english'] != '')]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Clean your DataFrame\n",
    "df_clean = clean_data(df)\n",
    "\n",
    "# 2. Modified Transformer Translator with better error handling\n",
    "class RobustTranslator(keras.Model):\n",
    "    def __init__(self, max_vocab_size=20000, max_length=50, embed_dim=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Swedish encoder with output mode='int'\n",
    "        self.sv_encoder = keras.layers.TextVectorization(\n",
    "            max_tokens=max_vocab_size,\n",
    "            output_sequence_length=max_length,\n",
    "            output_mode='int'\n",
    "        )\n",
    "        \n",
    "        # English decoder with output mode='int'\n",
    "        self.en_decoder = keras.layers.TextVectorization(\n",
    "            max_tokens=max_vocab_size,\n",
    "            output_sequence_length=max_length,\n",
    "            output_mode='int'\n",
    "        )\n",
    "        \n",
    "        # Embedding layers\n",
    "        self.sv_embedding = keras.layers.Embedding(max_vocab_size, embed_dim)\n",
    "        self.en_embedding = keras.layers.Embedding(max_vocab_size, embed_dim)\n",
    "        \n",
    "        # Encoder (BiLSTM)\n",
    "        self.encoder = keras.layers.Bidirectional(\n",
    "            keras.layers.LSTM(embed_dim, return_sequences=True))\n",
    "        \n",
    "        # Attention layer\n",
    "        self.attention = keras.layers.Attention()\n",
    "        \n",
    "        # Decoder (LSTM)\n",
    "        self.decoder = keras.layers.LSTM(embed_dim, return_sequences=True)\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = keras.layers.Dense(max_vocab_size, activation='softmax')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        sv_text, en_text = inputs\n",
    "        \n",
    "        # Encode Swedish\n",
    "        sv_tokens = self.sv_encoder(sv_text)\n",
    "        sv_emb = self.sv_embedding(sv_tokens)\n",
    "        encoded = self.encoder(sv_emb)\n",
    "        \n",
    "        # Decode English\n",
    "        en_tokens = self.en_decoder(en_text)\n",
    "        en_emb = self.en_embedding(en_tokens)\n",
    "        \n",
    "        # Attention\n",
    "        context = self.attention([en_emb, encoded])\n",
    "        \n",
    "        # Decode\n",
    "        decoded = self.decoder(context)\n",
    "        return self.output_layer(decoded)\n",
    "\n",
    "# 3. Prepare data\n",
    "train_df, val_df = train_test_split(df_clean, test_size=0.2)\n",
    "\n",
    "# Initialize model\n",
    "translator = RobustTranslator()\n",
    "\n",
    "# Adapt the vectorizers - now using .tolist() to ensure proper conversion\n",
    "translator.sv_encoder.adapt(train_df['swedish'].tolist())\n",
    "translator.en_decoder.adapt(train_df['english'].tolist())\n",
    "\n",
    "# 4. Data preparation with proper padding\n",
    "def prepare_data(sv_text, en_text):\n",
    "    # Convert to lists to avoid Tensor conversion issues\n",
    "    sv_text = sv_text.tolist()\n",
    "    en_text = en_text.tolist()\n",
    "    \n",
    "    # Get sequences\n",
    "    sv_seq = translator.sv_encoder(sv_text)\n",
    "    en_seq = translator.en_decoder(en_text)\n",
    "    \n",
    "    # Teacher forcing setup\n",
    "    return (sv_seq, en_seq[:, :-1]), en_seq[:, 1:]\n",
    "\n",
    "# Create datasets\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (train_df['swedish'], train_df['english'])\n",
    ").batch(32).map(prepare_data)\n",
    "\n",
    "val_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (val_df['swedish'], val_df['english'])\n",
    ").batch(32).map(prepare_data)\n",
    "\n",
    "# 5. Compile and train\n",
    "translator.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = translator.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cadd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892c8d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1ff972",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_nlp\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class TransformerTranslator(keras.Model):\n",
    "    def __init__(self, max_vocab_size=20000, max_length=50, embed_dim=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Swedish encoder\n",
    "        self.sv_encoder = keras.layers.TextVectorization(\n",
    "            max_tokens=max_vocab_size, output_sequence_length=max_length)\n",
    "        \n",
    "        # English decoder\n",
    "        self.en_decoder = keras.layers.TextVectorization(\n",
    "            max_tokens=max_vocab_size, output_sequence_length=max_length)\n",
    "        \n",
    "        # Embedding layers\n",
    "        self.sv_embedding = keras.layers.Embedding(max_vocab_size, embed_dim)\n",
    "        self.en_embedding = keras.layers.Embedding(max_vocab_size, embed_dim)\n",
    "        \n",
    "        # Transformer\n",
    "        self.transformer = keras_nlp.layers.TransformerEncoder(\n",
    "            num_heads=8,\n",
    "            intermediate_dim=512,\n",
    "            dropout=0.1\n",
    "        )\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = keras.layers.Dense(max_vocab_size, activation='softmax')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        sv_text, en_text = inputs\n",
    "        \n",
    "        # Encode Swedish\n",
    "        sv_tokens = self.sv_encoder(sv_text)\n",
    "        sv_emb = self.sv_embedding(sv_tokens)\n",
    "        encoded = self.transformer(sv_emb)\n",
    "        \n",
    "        # Decode English\n",
    "        en_tokens = self.en_decoder(en_text)\n",
    "        en_emb = self.en_embedding(en_tokens)\n",
    "        \n",
    "        # Simple cross-attention (for demo purposes)\n",
    "        attention = tf.matmul(en_emb, encoded, transpose_b=True)\n",
    "        attention = tf.nn.softmax(attention)\n",
    "        context = tf.matmul(attention, encoded)\n",
    "        \n",
    "        combined = tf.concat([en_emb, context], axis=-1)\n",
    "        return self.output_layer(combined)\n",
    "\n",
    "# Usage\n",
    "train_df, val_df = train_test_split(df, test_size=0.2)\n",
    "translator = TransformerTranslator()\n",
    "\n",
    "# Adapt the text vectorizers\n",
    "translator.sv_encoder.adapt(train_df['swedish'])\n",
    "translator.en_decoder.adapt(train_df['english'])\n",
    "\n",
    "# Compile and train\n",
    "translator.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Prepare data - need to shift decoder inputs for teacher forcing\n",
    "def prepare_data(sv_text, en_text):\n",
    "    # Convert texts to sequences\n",
    "    sv_seq = translator.sv_encoder(sv_text)\n",
    "    en_seq = translator.en_decoder(en_text)\n",
    "    \n",
    "    # For teacher forcing: decoder_inputs is en_seq[:-1], targets are en_seq[1:]\n",
    "    return (sv_seq, en_seq[:, :-1]), en_seq[:, 1:]\n",
    "\n",
    "# Create tf.data.Dataset\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (train_df['swedish'], train_df['english'])\n",
    ").batch(32).map(prepare_data)\n",
    "\n",
    "val_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (val_df['swedish'], val_df['english'])\n",
    ").batch(32).map(prepare_data)\n",
    "\n",
    "# Train\n",
    "translator.fit(train_ds, validation_data=val_ds, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db5eaa8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
