{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08162481",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e558736",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "\n",
    "# Parse the XML file\n",
    "tree = ET.parse('folkets_sv_en_public.xdxf.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "# Inspect the root to understand structure (optional)\n",
    "# print(ET.tostring(root, encoding='unicode'))\n",
    "\n",
    "# Extract data - adjust this depending on actual XML structure\n",
    "entries = []\n",
    "for ar in root.findall('.//ar'):  # 'ar' is the article tag in XDXF\n",
    "    term = ar.find('k').text if ar.find('k') is not None else None\n",
    "    definition = ''.join(ar.itertext()).strip()\n",
    "    entries.append({'term': term, 'definition': definition})\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(entries)\n",
    "\n",
    "# Show the first few rows\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bd8a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6956c94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the 'definition' column by '\\n' and expand into two new columns\n",
    "df[['swedish', 'english']] = df['definition'].str.split(r'\\n\\s+', n=1, expand=True)\n",
    "\n",
    "# Drop the old 'definition' column if you want\n",
    "df = df.drop(columns=['definition'])\n",
    "\n",
    "# Reorder columns if needed\n",
    "df = df[['term', 'swedish', 'english']]\n",
    "\n",
    "# View the cleaned DataFrame\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5312eda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Clean the 'english' column\n",
    "def clean_english(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    lines = text.split('\\n')\n",
    "    # Skip first 1-2 lines, keep the rest\n",
    "    cleaned = ' '.join(lines[2:]).strip()\n",
    "    return cleaned\n",
    "\n",
    "df['english_clean'] = df['english'].apply(clean_english)\n",
    "\n",
    "# View cleaned version\n",
    "df[['term', 'swedish', 'english_clean']].head()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa969563",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "def better_clean(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    lines = text.split('\\n')\n",
    "    # Remove lines that look like part of speech (like \"pp\", \"nn\", etc.)\n",
    "    lines = [line for line in lines if not re.match(r'^[a-z]{1,2}$', line.strip())]\n",
    "    # Remove lines that are pure IPA pronunciation (optional, if needed)\n",
    "    lines = [line for line in lines if not re.match(r\"^[a-zA-Zˈˌ:\\.\\s]+$\", line.strip())]\n",
    "    # Now, find the first line that seems like English (contains English words)\n",
    "    for line in lines:\n",
    "        if re.search(r'[a-zA-Z]', line) and not re.search(r'[åäöÅÄÖ]', line):\n",
    "            return line.strip()\n",
    "    return None\n",
    "\n",
    "df['english_clean'] = df['english'].apply(better_clean)\n",
    "\n",
    "# View results\n",
    "df[['term', 'swedish', 'english_clean']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d91508e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3626552",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2091ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def clean_dataframe(df):\n",
    "    # Reset index\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    # Custom cleaning function\n",
    "    def clean_text(text):\n",
    "        if pd.isna(text):\n",
    "            return text\n",
    "        # Keep only content before first \\n\n",
    "        cleaned = str(text).split('\\n')[0]\n",
    "        # Remove word tags at start (pp, nn, ab, etc.)\n",
    "        cleaned = re.sub(r'^\\w+\\s', '', cleaned)\n",
    "        return cleaned.strip()\n",
    "    \n",
    "    # Apply cleaning\n",
    "    df['english'] = df['english'].apply(clean_text)\n",
    "    \n",
    "    # Preserve good english_clean values\n",
    "    df['english'] = np.where(\n",
    "        df['english_clean'].notna() & (df['english_clean'] != df['english']),\n",
    "        df['english_clean'],\n",
    "        df['english']\n",
    "    )\n",
    "    df['english_clean'] = df['english']\n",
    "    \n",
    "    # Standardize\n",
    "    df['term'] = df['term'].str.lower()\n",
    "    df['swedish'] = df['swedish'].str.lower()\n",
    "    \n",
    "    return df.drop_duplicates(subset=['term'])\n",
    "\n",
    "# Apply cleaning\n",
    "df = clean_dataframe(df)\n",
    "\n",
    "# Ensure full content displays\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3aa918d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc380084",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b667b060",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9eb66d8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "term                                  abborre\n",
       "swedish                               abborre\n",
       "english          abborr|pinnevery small perch\n",
       "english_clean    abborr|pinnevery small perch\n",
       "Name: 13, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a87356",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b77d2c4e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParseError",
     "evalue": "not well-formed (invalid token): line 1, column 0 (<string>)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[0;32m\"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\"\u001b[0m, line \u001b[0;32m3457\u001b[0m, in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[0;32m\"C:\\Users\\enichol\\AppData\\Local\\Temp\\ipykernel_9616\\1054238443.py\"\u001b[0m, line \u001b[0;32m2\u001b[0m, in \u001b[0;35m<module>\u001b[0m\n    tree = ET.parse(\"en-sv.xml.gz\")\n",
      "  File \u001b[0;32m\"C:\\ProgramData\\Anaconda3\\lib\\xml\\etree\\ElementTree.py\"\u001b[0m, line \u001b[0;32m1224\u001b[0m, in \u001b[0;35mparse\u001b[0m\n    tree.parse(source, parser)\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\ProgramData\\Anaconda3\\lib\\xml\\etree\\ElementTree.py\"\u001b[1;36m, line \u001b[1;32m580\u001b[1;36m, in \u001b[1;35mparse\u001b[1;36m\u001b[0m\n\u001b[1;33m    self._root = parser._parse_whole(source)\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32m\"<string>\"\u001b[1;36m, line \u001b[1;32munknown\u001b[0m\n\u001b[1;31mParseError\u001b[0m\u001b[1;31m:\u001b[0m not well-formed (invalid token): line 1, column 0\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "tree = ET.parse(\"en-sv.xml.gz\")\n",
    "for seg in tree.findall(\".//seg\"):\n",
    "    print(seg.text)  # Extract Swedish/English text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d78aa45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Decompress the .xml.gz file\n",
    "with gzip.open(\"en-sv.xml.gz\", \"rb\") as gz_file:\n",
    "    with open(\"en-sv.xml\", \"wb\") as xml_file:\n",
    "        xml_file.write(gz_file.read())\n",
    "\n",
    "# Now parse the decompressed XML\n",
    "tree = ET.parse(\"en-sv.xml\")\n",
    "root = tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0bb866",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "import gzip\n",
    "\n",
    "with gzip.open(\"en-sv.xml.gz\", \"rb\") as f:\n",
    "    tree = etree.parse(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e7817a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3861bfb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
