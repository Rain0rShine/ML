{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73b0811-cc90-49b8-b4c0-9dc45f13c1ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dabbd0b0-a146-4626-bbab-a114481994d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6450f21-9509-4712-850d-de398f1b2c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('en-sv_translations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c9b4955-d6bb-461b-a373-67ebccc42202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>swedish</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Previously on The Hot Zone: Anthrax.</td>\n",
       "      <td>I tidigare avsnitt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Director Mueller just assigned us a major case...</td>\n",
       "      <td>Byråchef Mueller gav oss just ett stort fall.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Investigation''s  officially been dubbed Ameri...</td>\n",
       "      <td>Utredningen har fått namnet Amerithrax.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Whoever sent these  letters got their Anthrax ...</td>\n",
       "      <td>Brevskickaren fick sin mjältbrand från ett ame...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We wouldn''t be here if we didn''t have eviden...</td>\n",
       "      <td>Vi hade inte varit här om inte bevisen pekat p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43533706</th>\n",
       "      <td>You are already almost 15 minutes late. Oh, my...</td>\n",
       "      <td>-Gå nu, du är nästan en kvart sen.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43533707</th>\n",
       "      <td>By the powers vested in me by the state of Sou...</td>\n",
       "      <td>I kraft av mitt ämbete i staten South Carolina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43533708</th>\n",
       "      <td>Who invited you? - I'm-- - Beat it.</td>\n",
       "      <td>Vem bjöd in dig?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43533709</th>\n",
       "      <td>Okay.</td>\n",
       "      <td>! Stick!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43533710</th>\n",
       "      <td>You may now kiss your bride.</td>\n",
       "      <td>Ni kan kyssa er brud.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43533688 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    english  \\\n",
       "0                      Previously on The Hot Zone: Anthrax.   \n",
       "1         Director Mueller just assigned us a major case...   \n",
       "2         Investigation''s  officially been dubbed Ameri...   \n",
       "3         Whoever sent these  letters got their Anthrax ...   \n",
       "4         We wouldn''t be here if we didn''t have eviden...   \n",
       "...                                                     ...   \n",
       "43533706  You are already almost 15 minutes late. Oh, my...   \n",
       "43533707  By the powers vested in me by the state of Sou...   \n",
       "43533708                Who invited you? - I'm-- - Beat it.   \n",
       "43533709                                              Okay.   \n",
       "43533710                       You may now kiss your bride.   \n",
       "\n",
       "                                                    swedish  \n",
       "0                                     I tidigare avsnitt...  \n",
       "1             Byråchef Mueller gav oss just ett stort fall.  \n",
       "2                   Utredningen har fått namnet Amerithrax.  \n",
       "3         Brevskickaren fick sin mjältbrand från ett ame...  \n",
       "4         Vi hade inte varit här om inte bevisen pekat p...  \n",
       "...                                                     ...  \n",
       "43533706                 -Gå nu, du är nästan en kvart sen.  \n",
       "43533707  I kraft av mitt ämbete i staten South Carolina...  \n",
       "43533708                                   Vem bjöd in dig?  \n",
       "43533709                                           ! Stick!  \n",
       "43533710                              Ni kan kyssa er brud.  \n",
       "\n",
       "[43533688 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6525fdef-e8ca-4f2c-8bae-0ba0eee8cf10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\emma\\anaconda3\\lib\\site-packages (4.51.3)\n",
      "Requirement already satisfied: datasets in c:\\users\\emma\\anaconda3\\lib\\site-packages (3.5.1)\n",
      "Requirement already satisfied: tokenizers in c:\\users\\emma\\anaconda3\\lib\\site-packages (0.21.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\emma\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\emma\\anaconda3\\lib\\site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\emma\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\emma\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\emma\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\emma\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\emma\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\emma\\anaconda3\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\emma\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\emma\\anaconda3\\lib\\site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\emma\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\emma\\anaconda3\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in c:\\users\\emma\\anaconda3\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\emma\\anaconda3\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in c:\\users\\emma\\anaconda3\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\emma\\anaconda3\\lib\\site-packages (from datasets) (3.10.5)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\emma\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\emma\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\emma\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\emma\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\emma\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\emma\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.11.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\emma\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\emma\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\emma\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\emma\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\emma\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: colorama in c:\\users\\emma\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\emma\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\emma\\anaconda3\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\emma\\anaconda3\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\emma\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers datasets tokenizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b48c32-8c2f-449c-8904-f25acbbc2d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    PreTrainedTokenizerFast,\n",
    "    T5Config,\n",
    "    T5ForConditionalGeneration,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "\n",
    "# ===== 2. Train a Tokenizer from Scratch =====\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "# Write all text to a file\n",
    "with open(\"training_texts.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in df[\"english\"].fillna(\"\").astype(str).tolist() + df[\"swedish\"].fillna(\"\").astype(str).tolist(): \n",
    "        f.write(line.strip() + \"\\n\")\n",
    "\n",
    "\n",
    "# Train tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "tokenizer.train(files=\"training_texts.txt\", vocab_size=32000, min_frequency=2)\n",
    "\n",
    "# Save tokenizer\n",
    "os.makedirs(\"tokenizer\", exist_ok=True)\n",
    "tokenizer.save_model(\"tokenizer\")\n",
    "\n",
    "# Load tokenizer into Hugging Face\n",
    "hf_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"tokenizer/tokenizer.json\",\n",
    "    pad_token=\"<pad>\",\n",
    "    bos_token=\"<s>\",\n",
    "    eos_token=\"</s>\",\n",
    "    unk_token=\"<unk>\"\n",
    ")\n",
    "\n",
    "# ===== 3. Initialize T5 Model from Scratch =====\n",
    "config = T5Config(\n",
    "    vocab_size=hf_tokenizer.vocab_size,\n",
    "    d_model=512,\n",
    "    d_ff=2048,\n",
    "    num_layers=6,\n",
    "    num_heads=8,\n",
    "    dropout_rate=0.1,\n",
    "    eos_token_id=hf_tokenizer.eos_token_id,\n",
    "    pad_token_id=hf_tokenizer.pad_token_id,\n",
    "    decoder_start_token_id=hf_tokenizer.bos_token_id,\n",
    ")\n",
    "\n",
    "model = T5ForConditionalGeneration(config)\n",
    "\n",
    "# ===== 4. Tokenize the Dataset =====\n",
    "def preprocess_function(examples):\n",
    "    inputs = hf_tokenizer(\n",
    "        examples[\"english\"], truncation=True, padding=\"max_length\", max_length=128\n",
    "    )\n",
    "    targets = hf_tokenizer(\n",
    "        examples[\"swedish\"], truncation=True, padding=\"max_length\", max_length=128\n",
    "    )\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "split = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = split[\"train\"]\n",
    "val_dataset = split[\"test\"]\n",
    "\n",
    "# ===== 5. Training Setup =====\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./scratch_translation_model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-4,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    predict_with_generate=True,\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=hf_tokenizer, model=model)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=hf_tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# ===== 6. Train the Model =====\n",
    "trainer.train()\n",
    "\n",
    "# ===== 7. Save the Model =====\n",
    "model.save_pretrained(\"my_translation_model\")\n",
    "hf_tokenizer.save_pretrained(\"my_translation_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe6b917-5665-4cf5-81f6-249d99d30d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, PreTrainedTokenizerFast\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"my_translation_model\")\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"my_translation_model\")\n",
    "\n",
    "input_text = \"We wouldn't be here if we didn't have evidence.\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "output = model.generate(**inputs, max_length=50)\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2c92d8-cb9c-499c-bf2d-81f14109cd37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
