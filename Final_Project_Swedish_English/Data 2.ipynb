{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0d59f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7f60508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (1.4.4)\n",
      "Requirement already satisfied: lxml in c:\\programdata\\anaconda3\\lib\\site-packages (4.9.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas lxml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d07bfc64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 43533711 translation units\n",
      "                                             english  \\\n",
      "0               Previously on The Hot Zone: Anthrax.   \n",
      "1  Director Mueller just assigned us a major case...   \n",
      "2  Investigation''s  officially been dubbed Ameri...   \n",
      "3  Whoever sent these  letters got their Anthrax ...   \n",
      "4  We wouldn''t be here if we didn''t have eviden...   \n",
      "\n",
      "                                             swedish  \n",
      "0                              I tidigare avsnitt...  \n",
      "1      Byråchef Mueller gav oss just ett stort fall.  \n",
      "2            Utredningen har fått namnet Amerithrax.  \n",
      "3  Brevskickaren fick sin mjältbrand från ett ame...  \n",
      "4  Vi hade inte varit här om inte bevisen pekat p...  \n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import pandas as pd\n",
    "from lxml import etree\n",
    "\n",
    "def process_tmx_chunked(file_path, chunk_size=10000):\n",
    "    # Create an empty list to store DataFrames\n",
    "    chunks = []\n",
    "    \n",
    "    # Initialize variables for chunk processing\n",
    "    current_chunk = []\n",
    "    processed_count = 0\n",
    "    \n",
    "    # Use iterparse for memory-efficient parsing\n",
    "    context = etree.iterparse(\n",
    "        gzip.open(file_path), \n",
    "        events=('end',), \n",
    "        tag='tu'\n",
    "    )\n",
    "    \n",
    "    for event, elem in context:\n",
    "        try:\n",
    "            variants = {}\n",
    "            for tuv in elem.findall('tuv'):\n",
    "                lang = tuv.get('{http://www.w3.org/XML/1998/namespace}lang')\n",
    "                seg = tuv.find('seg')\n",
    "                variants[lang] = seg.text if seg is not None else ''\n",
    "            \n",
    "            if 'en' in variants and 'sv' in variants:\n",
    "                current_chunk.append({\n",
    "                    'english': variants['en'],\n",
    "                    'swedish': variants['sv']\n",
    "                })\n",
    "                processed_count += 1\n",
    "                \n",
    "                # When chunk is full, convert to DataFrame and store\n",
    "                if len(current_chunk) >= chunk_size:\n",
    "                    chunks.append(pd.DataFrame(current_chunk))\n",
    "                    current_chunk = []\n",
    "        finally:\n",
    "            # Clear memory after processing each element\n",
    "            elem.clear()\n",
    "            while elem.getprevious() is not None:\n",
    "                del elem.getparent()[0]\n",
    "    \n",
    "    # Add any remaining items in the last chunk\n",
    "    if current_chunk:\n",
    "        chunks.append(pd.DataFrame(current_chunk))\n",
    "    \n",
    "    # Combine all chunks into a single DataFrame\n",
    "    final_df = pd.concat(chunks, ignore_index=True)\n",
    "    \n",
    "    print(f\"Processed {processed_count} translation units\")\n",
    "    return final_df\n",
    "\n",
    "# Usage:\n",
    "df = process_tmx_chunked('en-sv.tmx.gz')\n",
    "print(df.head())  # Now this will work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9774b226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 43533711 entries, 0 to 43533710\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Dtype \n",
      "---  ------   ----- \n",
      " 0   english  object\n",
      " 1   swedish  object\n",
      "dtypes: object(2)\n",
      "memory usage: 664.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f061ed70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>swedish</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Previously on The Hot Zone: Anthrax.</td>\n",
       "      <td>I tidigare avsnitt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Director Mueller just assigned us a major case...</td>\n",
       "      <td>Byråchef Mueller gav oss just ett stort fall.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Investigation''s  officially been dubbed Ameri...</td>\n",
       "      <td>Utredningen har fått namnet Amerithrax.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Whoever sent these  letters got their Anthrax ...</td>\n",
       "      <td>Brevskickaren fick sin mjältbrand från ett ame...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We wouldn''t be here if we didn''t have eviden...</td>\n",
       "      <td>Vi hade inte varit här om inte bevisen pekat p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             english  \\\n",
       "0               Previously on The Hot Zone: Anthrax.   \n",
       "1  Director Mueller just assigned us a major case...   \n",
       "2  Investigation''s  officially been dubbed Ameri...   \n",
       "3  Whoever sent these  letters got their Anthrax ...   \n",
       "4  We wouldn''t be here if we didn''t have eviden...   \n",
       "\n",
       "                                             swedish  \n",
       "0                              I tidigare avsnitt...  \n",
       "1      Byråchef Mueller gav oss just ett stort fall.  \n",
       "2            Utredningen har fått namnet Amerithrax.  \n",
       "3  Brevskickaren fick sin mjältbrand från ett ame...  \n",
       "4  Vi hade inte varit här om inte bevisen pekat p...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32a973e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>swedish</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43533706</th>\n",
       "      <td>You are already almost 15 minutes late. Oh, my...</td>\n",
       "      <td>-Gå nu, du är nästan en kvart sen.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43533707</th>\n",
       "      <td>By the powers vested in me by the state of Sou...</td>\n",
       "      <td>I kraft av mitt ämbete i staten South Carolina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43533708</th>\n",
       "      <td>Who invited you? - I'm-- - Beat it.</td>\n",
       "      <td>Vem bjöd in dig?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43533709</th>\n",
       "      <td>Okay.</td>\n",
       "      <td>! Stick!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43533710</th>\n",
       "      <td>You may now kiss your bride.</td>\n",
       "      <td>Ni kan kyssa er brud.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    english  \\\n",
       "43533706  You are already almost 15 minutes late. Oh, my...   \n",
       "43533707  By the powers vested in me by the state of Sou...   \n",
       "43533708                Who invited you? - I'm-- - Beat it.   \n",
       "43533709                                              Okay.   \n",
       "43533710                       You may now kiss your bride.   \n",
       "\n",
       "                                                    swedish  \n",
       "43533706                 -Gå nu, du är nästan en kvart sen.  \n",
       "43533707  I kraft av mitt ämbete i staten South Carolina...  \n",
       "43533708                                   Vem bjöd in dig?  \n",
       "43533709                                           ! Stick!  \n",
       "43533710                              Ni kan kyssa er brud.  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56531587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "df.to_csv('en-sv_translations.csv', index=False)\n",
    "\n",
    "# Later load it with:\n",
    "df = pd.read_csv('en-sv_translations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c110d11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 43533711 entries, 0 to 43533710\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Dtype \n",
      "---  ------   ----- \n",
      " 0   english  object\n",
      " 1   swedish  object\n",
      "dtypes: object(2)\n",
      "memory usage: 664.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7d7ffd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>swedish</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Previously on The Hot Zone: Anthrax.</td>\n",
       "      <td>I tidigare avsnitt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Director Mueller just assigned us a major case...</td>\n",
       "      <td>Byråchef Mueller gav oss just ett stort fall.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Investigation''s  officially been dubbed Ameri...</td>\n",
       "      <td>Utredningen har fått namnet Amerithrax.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Whoever sent these  letters got their Anthrax ...</td>\n",
       "      <td>Brevskickaren fick sin mjältbrand från ett ame...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We wouldn''t be here if we didn''t have eviden...</td>\n",
       "      <td>Vi hade inte varit här om inte bevisen pekat p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43533706</th>\n",
       "      <td>You are already almost 15 minutes late. Oh, my...</td>\n",
       "      <td>-Gå nu, du är nästan en kvart sen.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43533707</th>\n",
       "      <td>By the powers vested in me by the state of Sou...</td>\n",
       "      <td>I kraft av mitt ämbete i staten South Carolina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43533708</th>\n",
       "      <td>Who invited you? - I'm-- - Beat it.</td>\n",
       "      <td>Vem bjöd in dig?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43533709</th>\n",
       "      <td>Okay.</td>\n",
       "      <td>! Stick!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43533710</th>\n",
       "      <td>You may now kiss your bride.</td>\n",
       "      <td>Ni kan kyssa er brud.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43533693 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    english  \\\n",
       "0                      Previously on The Hot Zone: Anthrax.   \n",
       "1         Director Mueller just assigned us a major case...   \n",
       "2         Investigation''s  officially been dubbed Ameri...   \n",
       "3         Whoever sent these  letters got their Anthrax ...   \n",
       "4         We wouldn''t be here if we didn''t have eviden...   \n",
       "...                                                     ...   \n",
       "43533706  You are already almost 15 minutes late. Oh, my...   \n",
       "43533707  By the powers vested in me by the state of Sou...   \n",
       "43533708                Who invited you? - I'm-- - Beat it.   \n",
       "43533709                                              Okay.   \n",
       "43533710                       You may now kiss your bride.   \n",
       "\n",
       "                                                    swedish  \n",
       "0                                     I tidigare avsnitt...  \n",
       "1             Byråchef Mueller gav oss just ett stort fall.  \n",
       "2                   Utredningen har fått namnet Amerithrax.  \n",
       "3         Brevskickaren fick sin mjältbrand från ett ame...  \n",
       "4         Vi hade inte varit här om inte bevisen pekat p...  \n",
       "...                                                     ...  \n",
       "43533706                 -Gå nu, du är nästan en kvart sen.  \n",
       "43533707  I kraft av mitt ämbete i staten South Carolina...  \n",
       "43533708                                   Vem bjöd in dig?  \n",
       "43533709                                           ! Stick!  \n",
       "43533710                              Ni kan kyssa er brud.  \n",
       "\n",
       "[43533693 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b496dac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1dae89a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: keras-nlp in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (0.20.0)\n",
      "Requirement already satisfied: keras-hub==0.20.0 in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from keras-nlp) (0.20.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from keras-hub==0.20.0->keras-nlp) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from keras-hub==0.20.0->keras-nlp) (21.3)\n",
      "Requirement already satisfied: rich in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from keras-hub==0.20.0->keras-nlp) (14.0.0)\n",
      "Requirement already satisfied: absl-py in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from keras-hub==0.20.0->keras-nlp) (2.2.2)\n",
      "Requirement already satisfied: keras>=3.5 in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from keras-hub==0.20.0->keras-nlp) (3.9.2)\n",
      "Requirement already satisfied: regex in c:\\programdata\\anaconda3\\lib\\site-packages (from keras-hub==0.20.0->keras-nlp) (2022.7.9)\n",
      "Requirement already satisfied: kagglehub in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from keras-hub==0.20.0->keras-nlp) (0.3.12)\n",
      "Requirement already satisfied: h5py in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from keras>=3.5->keras-hub==0.20.0->keras-nlp) (3.13.0)\n",
      "Requirement already satisfied: namex in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from keras>=3.5->keras-hub==0.20.0->keras-nlp) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from keras>=3.5->keras-hub==0.20.0->keras-nlp) (0.14.1)\n",
      "Requirement already satisfied: ml-dtypes in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from keras>=3.5->keras-hub==0.20.0->keras-nlp) (0.5.1)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from kagglehub->keras-hub==0.20.0->keras-nlp) (2.28.1)\n",
      "Requirement already satisfied: pyyaml in c:\\programdata\\anaconda3\\lib\\site-packages (from kagglehub->keras-hub==0.20.0->keras-nlp) (6.0)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from kagglehub->keras-hub==0.20.0->keras-nlp) (4.64.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging->keras-hub==0.20.0->keras-nlp) (3.0.9)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from rich->keras-hub==0.20.0->keras-nlp) (2.19.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from rich->keras-hub==0.20.0->keras-nlp) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from rich->keras-hub==0.20.0->keras-nlp) (4.13.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from markdown-it-py>=2.2.0->rich->keras-hub==0.20.0->keras-nlp) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->kagglehub->keras-hub==0.20.0->keras-nlp) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->kagglehub->keras-hub==0.20.0->keras-nlp) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->kagglehub->keras-hub==0.20.0->keras-nlp) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->kagglehub->keras-hub==0.20.0->keras-nlp) (3.3)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm->kagglehub->keras-hub==0.20.0->keras-nlp) (0.4.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1e3d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\enichol\\AppData\\Local\\Temp\\ipykernel_22896\\1363675162.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['swedish'] = df['swedish'].astype(str).str.strip()\n",
      "C:\\Users\\enichol\\AppData\\Local\\Temp\\ipykernel_22896\\1363675162.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['english'] = df['english'].astype(str).str.strip()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Clean the data first\n",
    "def clean_data(df):\n",
    "    # Remove rows with None/NaN in either column\n",
    "    df = df.dropna(subset=['swedish', 'english'])\n",
    "    \n",
    "    # Convert to string and strip whitespace\n",
    "    df['swedish'] = df['swedish'].astype(str).str.strip()\n",
    "    df['english'] = df['english'].astype(str).str.strip()\n",
    "    \n",
    "    # Remove empty strings\n",
    "    df = df[(df['swedish'] != '') & (df['english'] != '')]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Clean your DataFrame\n",
    "df_clean = clean_data(df)\n",
    "\n",
    "# 2. Modified Transformer Translator with better error handling\n",
    "class RobustTranslator(keras.Model):\n",
    "    def __init__(self, max_vocab_size=20000, max_length=50, embed_dim=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Swedish encoder with output mode='int'\n",
    "        self.sv_encoder = keras.layers.TextVectorization(\n",
    "            max_tokens=max_vocab_size,\n",
    "            output_sequence_length=max_length,\n",
    "            output_mode='int'\n",
    "        )\n",
    "        \n",
    "        # English decoder with output mode='int'\n",
    "        self.en_decoder = keras.layers.TextVectorization(\n",
    "            max_tokens=max_vocab_size,\n",
    "            output_sequence_length=max_length,\n",
    "            output_mode='int'\n",
    "        )\n",
    "        \n",
    "        # Embedding layers\n",
    "        self.sv_embedding = keras.layers.Embedding(max_vocab_size, embed_dim)\n",
    "        self.en_embedding = keras.layers.Embedding(max_vocab_size, embed_dim)\n",
    "        \n",
    "        # Encoder (BiLSTM)\n",
    "        self.encoder = keras.layers.Bidirectional(\n",
    "            keras.layers.LSTM(embed_dim, return_sequences=True))\n",
    "        \n",
    "        # Attention layer\n",
    "        self.attention = keras.layers.Attention()\n",
    "        \n",
    "        # Decoder (LSTM)\n",
    "        self.decoder = keras.layers.LSTM(embed_dim, return_sequences=True)\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = keras.layers.Dense(max_vocab_size, activation='softmax')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        sv_text, en_text = inputs\n",
    "        \n",
    "        # Encode Swedish\n",
    "        sv_tokens = self.sv_encoder(sv_text)\n",
    "        sv_emb = self.sv_embedding(sv_tokens)\n",
    "        encoded = self.encoder(sv_emb)\n",
    "        \n",
    "        # Decode English\n",
    "        en_tokens = self.en_decoder(en_text)\n",
    "        en_emb = self.en_embedding(en_tokens)\n",
    "        \n",
    "        # Attention\n",
    "        context = self.attention([en_emb, encoded])\n",
    "        \n",
    "        # Decode\n",
    "        decoded = self.decoder(context)\n",
    "        return self.output_layer(decoded)\n",
    "\n",
    "# 3. Prepare data\n",
    "train_df, val_df = train_test_split(df_clean, test_size=0.2)\n",
    "\n",
    "# Initialize model\n",
    "translator = RobustTranslator()\n",
    "\n",
    "# Adapt the vectorizers - now using .tolist() to ensure proper conversion\n",
    "translator.sv_encoder.adapt(train_df['swedish'].tolist())\n",
    "translator.en_decoder.adapt(train_df['english'].tolist())\n",
    "\n",
    "# 4. Data preparation with proper padding\n",
    "def prepare_data(sv_text, en_text):\n",
    "    # Convert to lists to avoid Tensor conversion issues\n",
    "    sv_text = sv_text.tolist()\n",
    "    en_text = en_text.tolist()\n",
    "    \n",
    "    # Get sequences\n",
    "    sv_seq = translator.sv_encoder(sv_text)\n",
    "    en_seq = translator.en_decoder(en_text)\n",
    "    \n",
    "    # Teacher forcing setup\n",
    "    return (sv_seq, en_seq[:, :-1]), en_seq[:, 1:]\n",
    "\n",
    "# Create datasets\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (train_df['swedish'], train_df['english'])\n",
    ").batch(32).map(prepare_data)\n",
    "\n",
    "val_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (val_df['swedish'], val_df['english'])\n",
    ").batch(32).map(prepare_data)\n",
    "\n",
    "# 5. Compile and train\n",
    "translator.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = translator.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4cadd03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>swedish</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Previously on The Hot Zone: Anthrax.</td>\n",
       "      <td>I tidigare avsnitt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Director Mueller just assigned us a major case...</td>\n",
       "      <td>Byråchef Mueller gav oss just ett stort fall.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Investigation''s  officially been dubbed Ameri...</td>\n",
       "      <td>Utredningen har fått namnet Amerithrax.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Whoever sent these  letters got their Anthrax ...</td>\n",
       "      <td>Brevskickaren fick sin mjältbrand från ett ame...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We wouldn''t be here if we didn''t have eviden...</td>\n",
       "      <td>Vi hade inte varit här om inte bevisen pekat p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43533706</th>\n",
       "      <td>You are already almost 15 minutes late. Oh, my...</td>\n",
       "      <td>-Gå nu, du är nästan en kvart sen.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43533707</th>\n",
       "      <td>By the powers vested in me by the state of Sou...</td>\n",
       "      <td>I kraft av mitt ämbete i staten South Carolina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43533708</th>\n",
       "      <td>Who invited you? - I'm-- - Beat it.</td>\n",
       "      <td>Vem bjöd in dig?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43533709</th>\n",
       "      <td>Okay.</td>\n",
       "      <td>! Stick!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43533710</th>\n",
       "      <td>You may now kiss your bride.</td>\n",
       "      <td>Ni kan kyssa er brud.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43533694 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    english  \\\n",
       "0                      Previously on The Hot Zone: Anthrax.   \n",
       "1         Director Mueller just assigned us a major case...   \n",
       "2         Investigation''s  officially been dubbed Ameri...   \n",
       "3         Whoever sent these  letters got their Anthrax ...   \n",
       "4         We wouldn''t be here if we didn''t have eviden...   \n",
       "...                                                     ...   \n",
       "43533706  You are already almost 15 minutes late. Oh, my...   \n",
       "43533707  By the powers vested in me by the state of Sou...   \n",
       "43533708                Who invited you? - I'm-- - Beat it.   \n",
       "43533709                                              Okay.   \n",
       "43533710                       You may now kiss your bride.   \n",
       "\n",
       "                                                    swedish  \n",
       "0                                     I tidigare avsnitt...  \n",
       "1             Byråchef Mueller gav oss just ett stort fall.  \n",
       "2                   Utredningen har fått namnet Amerithrax.  \n",
       "3         Brevskickaren fick sin mjältbrand från ett ame...  \n",
       "4         Vi hade inte varit här om inte bevisen pekat p...  \n",
       "...                                                     ...  \n",
       "43533706                 -Gå nu, du är nästan en kvart sen.  \n",
       "43533707  I kraft av mitt ämbete i staten South Carolina...  \n",
       "43533708                                   Vem bjöd in dig?  \n",
       "43533709                                           ! Stick!  \n",
       "43533710                              Ni kan kyssa er brud.  \n",
       "\n",
       "[43533694 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "892c8d3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>swedish</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Previously on The Hot Zone: Anthrax.</td>\n",
       "      <td>I tidigare avsnitt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Director Mueller just assigned us a major case...</td>\n",
       "      <td>Byråchef Mueller gav oss just ett stort fall.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Investigation''s  officially been dubbed Ameri...</td>\n",
       "      <td>Utredningen har fått namnet Amerithrax.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Whoever sent these  letters got their Anthrax ...</td>\n",
       "      <td>Brevskickaren fick sin mjältbrand från ett ame...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We wouldn''t be here if we didn''t have eviden...</td>\n",
       "      <td>Vi hade inte varit här om inte bevisen pekat p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43533706</th>\n",
       "      <td>You are already almost 15 minutes late. Oh, my...</td>\n",
       "      <td>-Gå nu, du är nästan en kvart sen.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43533707</th>\n",
       "      <td>By the powers vested in me by the state of Sou...</td>\n",
       "      <td>I kraft av mitt ämbete i staten South Carolina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43533708</th>\n",
       "      <td>Who invited you? - I'm-- - Beat it.</td>\n",
       "      <td>Vem bjöd in dig?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43533709</th>\n",
       "      <td>Okay.</td>\n",
       "      <td>! Stick!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43533710</th>\n",
       "      <td>You may now kiss your bride.</td>\n",
       "      <td>Ni kan kyssa er brud.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43533711 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    english  \\\n",
       "0                      Previously on The Hot Zone: Anthrax.   \n",
       "1         Director Mueller just assigned us a major case...   \n",
       "2         Investigation''s  officially been dubbed Ameri...   \n",
       "3         Whoever sent these  letters got their Anthrax ...   \n",
       "4         We wouldn''t be here if we didn''t have eviden...   \n",
       "...                                                     ...   \n",
       "43533706  You are already almost 15 minutes late. Oh, my...   \n",
       "43533707  By the powers vested in me by the state of Sou...   \n",
       "43533708                Who invited you? - I'm-- - Beat it.   \n",
       "43533709                                              Okay.   \n",
       "43533710                       You may now kiss your bride.   \n",
       "\n",
       "                                                    swedish  \n",
       "0                                     I tidigare avsnitt...  \n",
       "1             Byråchef Mueller gav oss just ett stort fall.  \n",
       "2                   Utredningen har fått namnet Amerithrax.  \n",
       "3         Brevskickaren fick sin mjältbrand från ett ame...  \n",
       "4         Vi hade inte varit här om inte bevisen pekat p...  \n",
       "...                                                     ...  \n",
       "43533706                 -Gå nu, du är nästan en kvart sen.  \n",
       "43533707  I kraft av mitt ämbete i staten South Carolina...  \n",
       "43533708                                   Vem bjöd in dig?  \n",
       "43533709                                           ! Stick!  \n",
       "43533710                              Ni kan kyssa er brud.  \n",
       "\n",
       "[43533711 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d1ff972",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type NoneType).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22896\\3399050282.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;31m# Adapt the text vectorizers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m \u001b[0mtranslator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msv_encoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madapt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'swedish'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[0mtranslator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0men_decoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madapt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\layers\\preprocessing\\text_vectorization.py\u001b[0m in \u001b[0;36madapt\u001b[1;34m(self, data, batch_size, steps)\u001b[0m\n\u001b[0;32m    421\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 423\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"string\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    424\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m                 \u001b[1;31m# A plain list of strings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\utils\\tf_utils.py\u001b[0m in \u001b[0;36mensure_tensor\u001b[1;34m(inputs, dtype)\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[1;31m# Plain `np.asarray()` conversion fails with PyTorch.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    106\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m   \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type NoneType)."
     ]
    }
   ],
   "source": [
    "import keras_nlp\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class TransformerTranslator(keras.Model):\n",
    "    def __init__(self, max_vocab_size=20000, max_length=50, embed_dim=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Swedish encoder\n",
    "        self.sv_encoder = keras.layers.TextVectorization(\n",
    "            max_tokens=max_vocab_size, output_sequence_length=max_length)\n",
    "        \n",
    "        # English decoder\n",
    "        self.en_decoder = keras.layers.TextVectorization(\n",
    "            max_tokens=max_vocab_size, output_sequence_length=max_length)\n",
    "        \n",
    "        # Embedding layers\n",
    "        self.sv_embedding = keras.layers.Embedding(max_vocab_size, embed_dim)\n",
    "        self.en_embedding = keras.layers.Embedding(max_vocab_size, embed_dim)\n",
    "        \n",
    "        # Transformer\n",
    "        self.transformer = keras_nlp.layers.TransformerEncoder(\n",
    "            num_heads=8,\n",
    "            intermediate_dim=512,\n",
    "            dropout=0.1\n",
    "        )\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = keras.layers.Dense(max_vocab_size, activation='softmax')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        sv_text, en_text = inputs\n",
    "        \n",
    "        # Encode Swedish\n",
    "        sv_tokens = self.sv_encoder(sv_text)\n",
    "        sv_emb = self.sv_embedding(sv_tokens)\n",
    "        encoded = self.transformer(sv_emb)\n",
    "        \n",
    "        # Decode English\n",
    "        en_tokens = self.en_decoder(en_text)\n",
    "        en_emb = self.en_embedding(en_tokens)\n",
    "        \n",
    "        # Simple cross-attention (for demo purposes)\n",
    "        attention = tf.matmul(en_emb, encoded, transpose_b=True)\n",
    "        attention = tf.nn.softmax(attention)\n",
    "        context = tf.matmul(attention, encoded)\n",
    "        \n",
    "        combined = tf.concat([en_emb, context], axis=-1)\n",
    "        return self.output_layer(combined)\n",
    "\n",
    "# Usage\n",
    "train_df, val_df = train_test_split(df, test_size=0.2)\n",
    "translator = TransformerTranslator()\n",
    "\n",
    "# Adapt the text vectorizers\n",
    "translator.sv_encoder.adapt(train_df['swedish'])\n",
    "translator.en_decoder.adapt(train_df['english'])\n",
    "\n",
    "# Compile and train\n",
    "translator.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Prepare data - need to shift decoder inputs for teacher forcing\n",
    "def prepare_data(sv_text, en_text):\n",
    "    # Convert texts to sequences\n",
    "    sv_seq = translator.sv_encoder(sv_text)\n",
    "    en_seq = translator.en_decoder(en_text)\n",
    "    \n",
    "    # For teacher forcing: decoder_inputs is en_seq[:-1], targets are en_seq[1:]\n",
    "    return (sv_seq, en_seq[:, :-1]), en_seq[:, 1:]\n",
    "\n",
    "# Create tf.data.Dataset\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (train_df['swedish'], train_df['english'])\n",
    ").batch(32).map(prepare_data)\n",
    "\n",
    "val_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (val_df['swedish'], val_df['english'])\n",
    ").batch(32).map(prepare_data)\n",
    "\n",
    "# Train\n",
    "translator.fit(train_ds, validation_data=val_ds, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db5eaa8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
