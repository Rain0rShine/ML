{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73b0811-cc90-49b8-b4c0-9dc45f13c1ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dabbd0b0-a146-4626-bbab-a114481994d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6450f21-9509-4712-850d-de398f1b2c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('en-sv_translations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c9b4955-d6bb-461b-a373-67ebccc42202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>swedish</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Previously on The Hot Zone: Anthrax.</td>\n",
       "      <td>I tidigare avsnitt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Director Mueller just assigned us a major case...</td>\n",
       "      <td>Byråchef Mueller gav oss just ett stort fall.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Investigation''s  officially been dubbed Ameri...</td>\n",
       "      <td>Utredningen har fått namnet Amerithrax.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Whoever sent these  letters got their Anthrax ...</td>\n",
       "      <td>Brevskickaren fick sin mjältbrand från ett ame...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We wouldn''t be here if we didn''t have eviden...</td>\n",
       "      <td>Vi hade inte varit här om inte bevisen pekat p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43533706</th>\n",
       "      <td>You are already almost 15 minutes late. Oh, my...</td>\n",
       "      <td>-Gå nu, du är nästan en kvart sen.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43533707</th>\n",
       "      <td>By the powers vested in me by the state of Sou...</td>\n",
       "      <td>I kraft av mitt ämbete i staten South Carolina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43533708</th>\n",
       "      <td>Who invited you? - I'm-- - Beat it.</td>\n",
       "      <td>Vem bjöd in dig?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43533709</th>\n",
       "      <td>Okay.</td>\n",
       "      <td>! Stick!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43533710</th>\n",
       "      <td>You may now kiss your bride.</td>\n",
       "      <td>Ni kan kyssa er brud.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43533688 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    english  \\\n",
       "0                      Previously on The Hot Zone: Anthrax.   \n",
       "1         Director Mueller just assigned us a major case...   \n",
       "2         Investigation''s  officially been dubbed Ameri...   \n",
       "3         Whoever sent these  letters got their Anthrax ...   \n",
       "4         We wouldn''t be here if we didn''t have eviden...   \n",
       "...                                                     ...   \n",
       "43533706  You are already almost 15 minutes late. Oh, my...   \n",
       "43533707  By the powers vested in me by the state of Sou...   \n",
       "43533708                Who invited you? - I'm-- - Beat it.   \n",
       "43533709                                              Okay.   \n",
       "43533710                       You may now kiss your bride.   \n",
       "\n",
       "                                                    swedish  \n",
       "0                                     I tidigare avsnitt...  \n",
       "1             Byråchef Mueller gav oss just ett stort fall.  \n",
       "2                   Utredningen har fått namnet Amerithrax.  \n",
       "3         Brevskickaren fick sin mjältbrand från ett ame...  \n",
       "4         Vi hade inte varit här om inte bevisen pekat p...  \n",
       "...                                                     ...  \n",
       "43533706                 -Gå nu, du är nästan en kvart sen.  \n",
       "43533707  I kraft av mitt ämbete i staten South Carolina...  \n",
       "43533708                                   Vem bjöd in dig?  \n",
       "43533709                                           ! Stick!  \n",
       "43533710                              Ni kan kyssa er brud.  \n",
       "\n",
       "[43533688 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dfb2a508-8bac-4d3a-86eb-33e1f5e2aa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=[\"english\", \"swedish\"])  # Drop missing values\n",
    "df[\"english\"] = df[\"english\"].astype(str)\n",
    "df[\"swedish\"] = df[\"swedish\"].astype(str)\n",
    "dataset = Dataset.from_pandas(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a79d031a-676a-4ff4-b669-8065c0493ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 43533688 entries, 0 to 43533710\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Dtype \n",
      "---  ------   ----- \n",
      " 0   english  object\n",
      " 1   swedish  object\n",
      "dtypes: object(2)\n",
      "memory usage: 996.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7f4aff65-c570-42c2-8c5a-96940399da53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 435337 entries, 39927259 to 28789177\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count   Dtype \n",
      "---  ------   --------------   ----- \n",
      " 0   english  435337 non-null  object\n",
      " 1   swedish  435337 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 10.0+ MB\n"
     ]
    }
   ],
   "source": [
    "small_df = df.sample(frac=0.01, random_state=42) \n",
    "df = small_df\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6525fdef-e8ca-4f2c-8bae-0ba0eee8cf10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\emma\\anaconda3\\lib\\site-packages (4.51.3)\n",
      "Requirement already satisfied: datasets in c:\\users\\emma\\anaconda3\\lib\\site-packages (3.5.1)\n",
      "Requirement already satisfied: tokenizers in c:\\users\\emma\\anaconda3\\lib\\site-packages (0.21.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\emma\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\emma\\anaconda3\\lib\\site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\emma\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\emma\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\emma\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\emma\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\emma\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\emma\\anaconda3\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\emma\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\emma\\anaconda3\\lib\\site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\emma\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\emma\\anaconda3\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in c:\\users\\emma\\anaconda3\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\emma\\anaconda3\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in c:\\users\\emma\\anaconda3\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\emma\\anaconda3\\lib\\site-packages (from datasets) (3.10.5)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\emma\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\emma\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\emma\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\emma\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\emma\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\emma\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.11.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\emma\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\emma\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\emma\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\emma\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\emma\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: colorama in c:\\users\\emma\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\emma\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\emma\\anaconda3\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\emma\\anaconda3\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\emma\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers datasets tokenizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "329df58a-188b-4ce1-9f1d-d6deb057378a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\emma\\anaconda3\\lib\\site-packages (4.51.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\emma\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\emma\\anaconda3\\lib\\site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\emma\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\emma\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\emma\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\emma\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\emma\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\emma\\anaconda3\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\emma\\anaconda3\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\emma\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\emma\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\emma\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\emma\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\emma\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\emma\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\emma\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\emma\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.4.26)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5ff11c0c-3b55-4ff9-9619-9d20a9418e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.51.3\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "35b48c32-8c2f-449c-8904-f25acbbc2d37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "419f833c994a4ce5ac7a881487489d7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/435337 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Emma\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Seq2SeqTrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 121\u001b[0m\n\u001b[0;32m    118\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m split[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    120\u001b[0m \u001b[38;5;66;03m# ===== 5. Training Setup =====\u001b[39;00m\n\u001b[1;32m--> 121\u001b[0m training_args \u001b[38;5;241m=\u001b[39m Seq2SeqTrainingArguments(\n\u001b[0;32m    122\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./scratch_translation_model\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    123\u001b[0m     evaluation_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    124\u001b[0m     save_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    125\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3e-4\u001b[39m,\n\u001b[0;32m    126\u001b[0m     per_device_train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[0;32m    127\u001b[0m     per_device_eval_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[0;32m    128\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m    129\u001b[0m     weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m,\n\u001b[0;32m    130\u001b[0m     save_total_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m    131\u001b[0m     predict_with_generate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    132\u001b[0m     logging_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./logs\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    133\u001b[0m )\n\u001b[0;32m    135\u001b[0m data_collator \u001b[38;5;241m=\u001b[39m DataCollatorForSeq2Seq(tokenizer\u001b[38;5;241m=\u001b[39mhf_tokenizer, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m    137\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Seq2SeqTrainer(\n\u001b[0;32m    138\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    139\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    143\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[0;32m    144\u001b[0m )\n",
      "\u001b[1;31mTypeError\u001b[0m: Seq2SeqTrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "from transformers import (\n",
    "    PreTrainedTokenizerFast,\n",
    "    T5Config,\n",
    "    T5ForConditionalGeneration,\n",
    "    Seq2SeqTrainer,\n",
    "    #Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "\n",
    "# ===== 2. Train a Tokenizer from Scratch =====\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "# Write all text to a file\n",
    "with open(\"training_texts.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in df[\"english\"].fillna(\"\").astype(str).tolist() + df[\"swedish\"].fillna(\"\").astype(str).tolist(): \n",
    "        f.write(line.strip() + \"\\n\")\n",
    "\n",
    "\n",
    "# Train tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "tokenizer.train(files=\"training_texts.txt\", vocab_size=32000, min_frequency=2)\n",
    "\n",
    "# Save tokenizer\n",
    "os.makedirs(\"tokenizer\", exist_ok=True)\n",
    "tokenizer.save_model(\"tokenizer\")\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"add_prefix_space\": True,\n",
    "    \"unk_token\": \"<unk>\",\n",
    "    \"bos_token\": \"<s>\",\n",
    "    \"eos_token\": \"</s>\",\n",
    "    \"pad_token\": \"<pad>\",\n",
    "    \"mask_token\": \"<mask>\"\n",
    "}\n",
    "\n",
    "with open(\"tokenizer/tokenizer_config.json\", \"w\") as f:\n",
    "    json.dump(config, f)\n",
    "\n",
    "\n",
    "\n",
    "# Load tokenizer into Hugging Face\n",
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "hf_tokenizer = GPT2TokenizerFast.from_pretrained(\n",
    "    \"tokenizer\",\n",
    "    pad_token=\"<pad>\",\n",
    "    bos_token=\"<s>\",\n",
    "    eos_token=\"</s>\",\n",
    "    unk_token=\"<unk>\"\n",
    ")\n",
    "\n",
    "\n",
    "from transformers import T5Config, T5ForConditionalGeneration\n",
    "\n",
    "# ===== 3. Initialize T5 Model from Scratch =====\n",
    "config = T5Config(\n",
    "    vocab_size=hf_tokenizer.vocab_size,\n",
    "    d_model=512,\n",
    "    d_ff=2048,\n",
    "    num_layers=6,\n",
    "    num_heads=8,\n",
    "    dropout_rate=0.1,\n",
    "    eos_token_id=hf_tokenizer.eos_token_id,\n",
    "    pad_token_id=hf_tokenizer.pad_token_id,\n",
    "    decoder_start_token_id=hf_tokenizer.bos_token_id,\n",
    ")\n",
    "\n",
    "model = T5ForConditionalGeneration(config)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ===== 4. Tokenize the Dataset =====\n",
    "def preprocess_function(examples):\n",
    "    english_texts = [str(text) for text in examples[\"english\"]]\n",
    "    swedish_texts = [str(text) for text in examples[\"swedish\"]]\n",
    "\n",
    "    # Ensure they are lists of strings\n",
    "    assert isinstance(english_texts, list), \"english_texts must be a list\"\n",
    "    assert isinstance(swedish_texts, list), \"swedish_texts must be a list\"\n",
    "\n",
    "    model_inputs = hf_tokenizer(\n",
    "        english_texts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "    )\n",
    "\n",
    "    with hf_tokenizer.as_target_tokenizer():\n",
    "        labels = hf_tokenizer(\n",
    "            swedish_texts,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "        )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "\n",
    "from datasets import Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "split = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = split[\"train\"]\n",
    "val_dataset = split[\"test\"]\n",
    "\n",
    "# ===== 5. Training Setup =====\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./scratch_translation_model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-4,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    predict_with_generate=True,\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=hf_tokenizer, model=model)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=hf_tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# ===== 6. Train the Model =====\n",
    "trainer.train()\n",
    "\n",
    "# ===== 7. Save the Model =====\n",
    "model.save_pretrained(\"my_translation_model\")\n",
    "hf_tokenizer.save_pretrained(\"my_translation_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe6b917-5665-4cf5-81f6-249d99d30d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, PreTrainedTokenizerFast\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"my_translation_model\")\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"my_translation_model\")\n",
    "\n",
    "input_text = \"We wouldn't be here if we didn't have evidence.\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "output = model.generate(**inputs, max_length=50)\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2c92d8-cb9c-499c-bf2d-81f14109cd37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
