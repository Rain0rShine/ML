{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c75eab7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f651e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12676b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('en-sv_translations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb8e45ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 43533711 entries, 0 to 43533710\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Dtype \n",
      "---  ------   ----- \n",
      " 0   english  object\n",
      " 1   swedish  object\n",
      "dtypes: object(2)\n",
      "memory usage: 664.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1549f5e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>swedish</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Previously on The Hot Zone: Anthrax.</td>\n",
       "      <td>I tidigare avsnitt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Director Mueller just assigned us a major case...</td>\n",
       "      <td>Byråchef Mueller gav oss just ett stort fall.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Investigation''s  officially been dubbed Ameri...</td>\n",
       "      <td>Utredningen har fått namnet Amerithrax.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Whoever sent these  letters got their Anthrax ...</td>\n",
       "      <td>Brevskickaren fick sin mjältbrand från ett ame...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We wouldn''t be here if we didn''t have eviden...</td>\n",
       "      <td>Vi hade inte varit här om inte bevisen pekat p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43533706</th>\n",
       "      <td>You are already almost 15 minutes late. Oh, my...</td>\n",
       "      <td>-Gå nu, du är nästan en kvart sen.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43533707</th>\n",
       "      <td>By the powers vested in me by the state of Sou...</td>\n",
       "      <td>I kraft av mitt ämbete i staten South Carolina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43533708</th>\n",
       "      <td>Who invited you? - I'm-- - Beat it.</td>\n",
       "      <td>Vem bjöd in dig?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43533709</th>\n",
       "      <td>Okay.</td>\n",
       "      <td>! Stick!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43533710</th>\n",
       "      <td>You may now kiss your bride.</td>\n",
       "      <td>Ni kan kyssa er brud.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43533693 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    english  \\\n",
       "0                      Previously on The Hot Zone: Anthrax.   \n",
       "1         Director Mueller just assigned us a major case...   \n",
       "2         Investigation''s  officially been dubbed Ameri...   \n",
       "3         Whoever sent these  letters got their Anthrax ...   \n",
       "4         We wouldn''t be here if we didn''t have eviden...   \n",
       "...                                                     ...   \n",
       "43533706  You are already almost 15 minutes late. Oh, my...   \n",
       "43533707  By the powers vested in me by the state of Sou...   \n",
       "43533708                Who invited you? - I'm-- - Beat it.   \n",
       "43533709                                              Okay.   \n",
       "43533710                       You may now kiss your bride.   \n",
       "\n",
       "                                                    swedish  \n",
       "0                                     I tidigare avsnitt...  \n",
       "1             Byråchef Mueller gav oss just ett stort fall.  \n",
       "2                   Utredningen har fått namnet Amerithrax.  \n",
       "3         Brevskickaren fick sin mjältbrand från ett ame...  \n",
       "4         Vi hade inte varit här om inte bevisen pekat p...  \n",
       "...                                                     ...  \n",
       "43533706                 -Gå nu, du är nästan en kvart sen.  \n",
       "43533707  I kraft av mitt ämbete i staten South Carolina...  \n",
       "43533708                                   Vem bjöd in dig?  \n",
       "43533709                                           ! Stick!  \n",
       "43533710                              Ni kan kyssa er brud.  \n",
       "\n",
       "[43533693 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23fc12d4",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>swedish</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Previously on The Hot Zone: Anthrax.</td>\n",
       "      <td>I tidigare avsnitt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Director Mueller just assigned us a major case...</td>\n",
       "      <td>Byråchef Mueller gav oss just ett stort fall.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Investigation''s  officially been dubbed Ameri...</td>\n",
       "      <td>Utredningen har fått namnet Amerithrax.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Whoever sent these  letters got their Anthrax ...</td>\n",
       "      <td>Brevskickaren fick sin mjältbrand från ett ame...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We wouldn''t be here if we didn''t have eviden...</td>\n",
       "      <td>Vi hade inte varit här om inte bevisen pekat p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43533706</th>\n",
       "      <td>You are already almost 15 minutes late. Oh, my...</td>\n",
       "      <td>-Gå nu, du är nästan en kvart sen.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43533707</th>\n",
       "      <td>By the powers vested in me by the state of Sou...</td>\n",
       "      <td>I kraft av mitt ämbete i staten South Carolina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43533708</th>\n",
       "      <td>Who invited you? - I'm-- - Beat it.</td>\n",
       "      <td>Vem bjöd in dig?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43533709</th>\n",
       "      <td>Okay.</td>\n",
       "      <td>! Stick!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43533710</th>\n",
       "      <td>You may now kiss your bride.</td>\n",
       "      <td>Ni kan kyssa er brud.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43533711 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    english  \\\n",
       "0                      Previously on The Hot Zone: Anthrax.   \n",
       "1         Director Mueller just assigned us a major case...   \n",
       "2         Investigation''s  officially been dubbed Ameri...   \n",
       "3         Whoever sent these  letters got their Anthrax ...   \n",
       "4         We wouldn''t be here if we didn''t have eviden...   \n",
       "...                                                     ...   \n",
       "43533706  You are already almost 15 minutes late. Oh, my...   \n",
       "43533707  By the powers vested in me by the state of Sou...   \n",
       "43533708                Who invited you? - I'm-- - Beat it.   \n",
       "43533709                                              Okay.   \n",
       "43533710                       You may now kiss your bride.   \n",
       "\n",
       "                                                    swedish  \n",
       "0                                     I tidigare avsnitt...  \n",
       "1             Byråchef Mueller gav oss just ett stort fall.  \n",
       "2                   Utredningen har fått namnet Amerithrax.  \n",
       "3         Brevskickaren fick sin mjältbrand från ett ame...  \n",
       "4         Vi hade inte varit här om inte bevisen pekat p...  \n",
       "...                                                     ...  \n",
       "43533706                 -Gå nu, du är nästan en kvart sen.  \n",
       "43533707  I kraft av mitt ämbete i staten South Carolina...  \n",
       "43533708                                   Vem bjöd in dig?  \n",
       "43533709                                           ! Stick!  \n",
       "43533710                              Ni kan kyssa er brud.  \n",
       "\n",
       "[43533711 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ac3224",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc627b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For initial experiments, you might want to work with a smaller subset\n",
    "sample_df = df.sample(frac=0.1, random_state=42)  # 10% sample\n",
    "\n",
    "# Split into train and test sets\n",
    "train_df, test_df = train_test_split(sample_df, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fcd204e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (2.19.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (3.9.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (3.0.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (5.29.4)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (0.31.0)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (63.4.1)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (2.2.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (4.13.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (1.71.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (0.5.1)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (21.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: namex in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
      "Requirement already satisfied: rich in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from keras>=3.5.0->tensorflow) (14.0.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2022.9.14)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging->tensorflow) (3.0.9)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
    "\n",
    "# Install TensorFlow if needed\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8261afb4",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (4.51.3)\n",
      "Requirement already satisfied: datasets in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (3.5.1)\n",
      "Requirement already satisfied: torch in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (2.6.0)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (0.2.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (1.4.4)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from datasets) (3.11.18)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: fsspec[http]<=2025.3.0,>=2023.1.0 in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from datasets) (2025.3.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from torch) (4.13.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.2.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from aiohttp->datasets) (6.4.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from aiohttp->datasets) (1.20.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from aiohttp->datasets) (1.6.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.5)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers datasets torch sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b205d24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f3e6e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\enichol\\AppData\\Roaming\\Python\\Python39\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ac181a3830a4bbc8dd0e0d07b70f547",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\enichol\\AppData\\Roaming\\Python\\Python39\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\enichol\\.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-sv-en. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c58aca8e47b74241a1084abfcdf97e97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.38k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a77d7d7a455f456f8761ddc99f243881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "source.spm:   0%|          | 0.00/815k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82b9823e6dde4a1881c946752aa74527",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "target.spm:   0%|          | 0.00/790k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6581a4a160544c5aa3143ca047c1229",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\enichol\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43e8c37f26a74d79823287750069eb3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/295M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "418101321a574cf8bb78fada6c8cc161",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74732f2824584254b3d5878a47525446",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/294M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "model_checkpoint = \"Helsinki-NLP/opus-mt-sv-en\"  # Pretrained Swedish-English model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf7b812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (4.51.3)\n",
      "Requirement already satisfied: datasets in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (3.5.1)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (1.4.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (1.0.2)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: requests in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec[http]<=2025.3.0,>=2023.1.0 in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from datasets) (2025.3.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from datasets) (3.11.18)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from aiohttp->datasets) (6.4.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from aiohttp->datasets) (1.20.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from aiohttp->datasets) (1.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.5)\n"
     ]
    }
   ],
   "source": [
    "# First install required packages if you haven't\n",
    "!pip install transformers datasets pandas scikit-learn\n",
    "\n",
    "# Now the full code with all imports\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset  # This is the missing import\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "\n",
    "# 1. Load your data\n",
    "try:\n",
    "    df = pd.read_csv('en-sv_translations.csv')\n",
    "    print(f\"Data loaded successfully with {len(df)} rows\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: File not found. Please check the file path.\")\n",
    "\n",
    "# For initial experiments, you might want to work with a smaller subset\n",
    "sample_df = df.sample(frac=0.1, random_state=42)  # 10% sample\n",
    "\n",
    "# Split into train and test sets\n",
    "train_df, test_df = train_test_split(sample_df, test_size=0.1, random_state=42)\n",
    "\n",
    "# 5. Convert to HuggingFace Dataset format\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "print(\"Datasets prepared successfully!\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(test_dataset)}\")\n",
    "\n",
    "# Rest of your code can follow..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ab82eec",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_27184\\2953231714.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# Apply to datasets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mtokenized_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreprocess_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatched\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mtokenized_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreprocess_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatched\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = [ex['swedish'] for ex in examples]\n",
    "    targets = [ex['english'] for ex in examples]\n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True)\n",
    "    \n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=128, truncation=True)\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Apply to datasets\n",
    "tokenized_train = train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_val = val_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56ca403",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
