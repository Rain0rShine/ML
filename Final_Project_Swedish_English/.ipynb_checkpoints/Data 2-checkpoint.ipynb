{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0d59f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7f60508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (1.4.4)\n",
      "Requirement already satisfied: lxml in c:\\programdata\\anaconda3\\lib\\site-packages (4.9.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas lxml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d07bfc64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 43533711 translation units\n",
      "                                             english  \\\n",
      "0               Previously on The Hot Zone: Anthrax.   \n",
      "1  Director Mueller just assigned us a major case...   \n",
      "2  Investigation''s  officially been dubbed Ameri...   \n",
      "3  Whoever sent these  letters got their Anthrax ...   \n",
      "4  We wouldn''t be here if we didn''t have eviden...   \n",
      "\n",
      "                                             swedish  \n",
      "0                              I tidigare avsnitt...  \n",
      "1      Byråchef Mueller gav oss just ett stort fall.  \n",
      "2            Utredningen har fått namnet Amerithrax.  \n",
      "3  Brevskickaren fick sin mjältbrand från ett ame...  \n",
      "4  Vi hade inte varit här om inte bevisen pekat p...  \n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import pandas as pd\n",
    "from lxml import etree\n",
    "\n",
    "def process_tmx_chunked(file_path, chunk_size=10000):\n",
    "    # Create an empty list to store DataFrames\n",
    "    chunks = []\n",
    "    \n",
    "    # Initialize variables for chunk processing\n",
    "    current_chunk = []\n",
    "    processed_count = 0\n",
    "    \n",
    "    # Use iterparse for memory-efficient parsing\n",
    "    context = etree.iterparse(\n",
    "        gzip.open(file_path), \n",
    "        events=('end',), \n",
    "        tag='tu'\n",
    "    )\n",
    "    \n",
    "    for event, elem in context:\n",
    "        try:\n",
    "            variants = {}\n",
    "            for tuv in elem.findall('tuv'):\n",
    "                lang = tuv.get('{http://www.w3.org/XML/1998/namespace}lang')\n",
    "                seg = tuv.find('seg')\n",
    "                variants[lang] = seg.text if seg is not None else ''\n",
    "            \n",
    "            if 'en' in variants and 'sv' in variants:\n",
    "                current_chunk.append({\n",
    "                    'english': variants['en'],\n",
    "                    'swedish': variants['sv']\n",
    "                })\n",
    "                processed_count += 1\n",
    "                \n",
    "                # When chunk is full, convert to DataFrame and store\n",
    "                if len(current_chunk) >= chunk_size:\n",
    "                    chunks.append(pd.DataFrame(current_chunk))\n",
    "                    current_chunk = []\n",
    "        finally:\n",
    "            # Clear memory after processing each element\n",
    "            elem.clear()\n",
    "            while elem.getprevious() is not None:\n",
    "                del elem.getparent()[0]\n",
    "    \n",
    "    # Add any remaining items in the last chunk\n",
    "    if current_chunk:\n",
    "        chunks.append(pd.DataFrame(current_chunk))\n",
    "    \n",
    "    # Combine all chunks into a single DataFrame\n",
    "    final_df = pd.concat(chunks, ignore_index=True)\n",
    "    \n",
    "    print(f\"Processed {processed_count} translation units\")\n",
    "    return final_df\n",
    "\n",
    "# Usage:\n",
    "df = process_tmx_chunked('en-sv.tmx.gz')\n",
    "print(df.head())  # Now this will work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9774b226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 43533711 entries, 0 to 43533710\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Dtype \n",
      "---  ------   ----- \n",
      " 0   english  object\n",
      " 1   swedish  object\n",
      "dtypes: object(2)\n",
      "memory usage: 664.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f061ed70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>swedish</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Previously on The Hot Zone: Anthrax.</td>\n",
       "      <td>I tidigare avsnitt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Director Mueller just assigned us a major case...</td>\n",
       "      <td>Byråchef Mueller gav oss just ett stort fall.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Investigation''s  officially been dubbed Ameri...</td>\n",
       "      <td>Utredningen har fått namnet Amerithrax.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Whoever sent these  letters got their Anthrax ...</td>\n",
       "      <td>Brevskickaren fick sin mjältbrand från ett ame...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We wouldn''t be here if we didn''t have eviden...</td>\n",
       "      <td>Vi hade inte varit här om inte bevisen pekat p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             english  \\\n",
       "0               Previously on The Hot Zone: Anthrax.   \n",
       "1  Director Mueller just assigned us a major case...   \n",
       "2  Investigation''s  officially been dubbed Ameri...   \n",
       "3  Whoever sent these  letters got their Anthrax ...   \n",
       "4  We wouldn''t be here if we didn''t have eviden...   \n",
       "\n",
       "                                             swedish  \n",
       "0                              I tidigare avsnitt...  \n",
       "1      Byråchef Mueller gav oss just ett stort fall.  \n",
       "2            Utredningen har fått namnet Amerithrax.  \n",
       "3  Brevskickaren fick sin mjältbrand från ett ame...  \n",
       "4  Vi hade inte varit här om inte bevisen pekat p...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32a973e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22048\\281403043.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtail\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b496dac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1dae89a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting keras-nlp\n",
      "  Downloading keras_nlp-0.20.0-py3-none-any.whl (2.1 kB)\n",
      "Collecting keras-hub==0.20.0\n",
      "  Downloading keras_hub-0.20.0-py3-none-any.whl (792 kB)\n",
      "     -------------------------------------- 792.1/792.1 kB 2.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: absl-py in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from keras-hub==0.20.0->keras-nlp) (2.2.2)\n",
      "Requirement already satisfied: keras>=3.5 in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from keras-hub==0.20.0->keras-nlp) (3.9.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from keras-hub==0.20.0->keras-nlp) (1.26.4)\n",
      "Requirement already satisfied: regex in c:\\programdata\\anaconda3\\lib\\site-packages (from keras-hub==0.20.0->keras-nlp) (2022.7.9)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from keras-hub==0.20.0->keras-nlp) (21.3)\n",
      "Collecting kagglehub\n",
      "  Downloading kagglehub-0.3.12-py3-none-any.whl (67 kB)\n",
      "     ---------------------------------------- 68.0/68.0 kB ? eta 0:00:00\n",
      "Requirement already satisfied: rich in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from keras-hub==0.20.0->keras-nlp) (14.0.0)\n",
      "Requirement already satisfied: namex in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from keras>=3.5->keras-hub==0.20.0->keras-nlp) (0.0.8)\n",
      "Requirement already satisfied: h5py in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from keras>=3.5->keras-hub==0.20.0->keras-nlp) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from keras>=3.5->keras-hub==0.20.0->keras-nlp) (0.5.1)\n",
      "Requirement already satisfied: optree in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from keras>=3.5->keras-hub==0.20.0->keras-nlp) (0.14.1)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from kagglehub->keras-hub==0.20.0->keras-nlp) (2.28.1)\n",
      "Requirement already satisfied: pyyaml in c:\\programdata\\anaconda3\\lib\\site-packages (from kagglehub->keras-hub==0.20.0->keras-nlp) (6.0)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from kagglehub->keras-hub==0.20.0->keras-nlp) (4.64.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging->keras-hub==0.20.0->keras-nlp) (3.0.9)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from rich->keras-hub==0.20.0->keras-nlp) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from rich->keras-hub==0.20.0->keras-nlp) (2.19.1)\n",
      "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from rich->keras-hub==0.20.0->keras-nlp) (4.13.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\enichol\\appdata\\roaming\\python\\python39\\site-packages (from markdown-it-py>=2.2.0->rich->keras-hub==0.20.0->keras-nlp) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->kagglehub->keras-hub==0.20.0->keras-nlp) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->kagglehub->keras-hub==0.20.0->keras-nlp) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->kagglehub->keras-hub==0.20.0->keras-nlp) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->kagglehub->keras-hub==0.20.0->keras-nlp) (1.26.11)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm->kagglehub->keras-hub==0.20.0->keras-nlp) (0.4.5)\n",
      "Installing collected packages: kagglehub, keras-hub, keras-nlp\n",
      "Successfully installed kagglehub-0.3.12 keras-hub-0.20.0 keras-nlp-0.20.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad1e3d63",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22048\\1363675162.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m# Clean your DataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mdf_clean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclean_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# 2. Modified Transformer Translator with better error handling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Clean the data first\n",
    "def clean_data(df):\n",
    "    # Remove rows with None/NaN in either column\n",
    "    df = df.dropna(subset=['swedish', 'english'])\n",
    "    \n",
    "    # Convert to string and strip whitespace\n",
    "    df['swedish'] = df['swedish'].astype(str).str.strip()\n",
    "    df['english'] = df['english'].astype(str).str.strip()\n",
    "    \n",
    "    # Remove empty strings\n",
    "    df = df[(df['swedish'] != '') & (df['english'] != '')]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Clean your DataFrame\n",
    "df_clean = clean_data(df)\n",
    "\n",
    "# 2. Modified Transformer Translator with better error handling\n",
    "class RobustTranslator(keras.Model):\n",
    "    def __init__(self, max_vocab_size=20000, max_length=50, embed_dim=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Swedish encoder with output mode='int'\n",
    "        self.sv_encoder = keras.layers.TextVectorization(\n",
    "            max_tokens=max_vocab_size,\n",
    "            output_sequence_length=max_length,\n",
    "            output_mode='int'\n",
    "        )\n",
    "        \n",
    "        # English decoder with output mode='int'\n",
    "        self.en_decoder = keras.layers.TextVectorization(\n",
    "            max_tokens=max_vocab_size,\n",
    "            output_sequence_length=max_length,\n",
    "            output_mode='int'\n",
    "        )\n",
    "        \n",
    "        # Embedding layers\n",
    "        self.sv_embedding = keras.layers.Embedding(max_vocab_size, embed_dim)\n",
    "        self.en_embedding = keras.layers.Embedding(max_vocab_size, embed_dim)\n",
    "        \n",
    "        # Encoder (BiLSTM)\n",
    "        self.encoder = keras.layers.Bidirectional(\n",
    "            keras.layers.LSTM(embed_dim, return_sequences=True))\n",
    "        \n",
    "        # Attention layer\n",
    "        self.attention = keras.layers.Attention()\n",
    "        \n",
    "        # Decoder (LSTM)\n",
    "        self.decoder = keras.layers.LSTM(embed_dim, return_sequences=True)\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = keras.layers.Dense(max_vocab_size, activation='softmax')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        sv_text, en_text = inputs\n",
    "        \n",
    "        # Encode Swedish\n",
    "        sv_tokens = self.sv_encoder(sv_text)\n",
    "        sv_emb = self.sv_embedding(sv_tokens)\n",
    "        encoded = self.encoder(sv_emb)\n",
    "        \n",
    "        # Decode English\n",
    "        en_tokens = self.en_decoder(en_text)\n",
    "        en_emb = self.en_embedding(en_tokens)\n",
    "        \n",
    "        # Attention\n",
    "        context = self.attention([en_emb, encoded])\n",
    "        \n",
    "        # Decode\n",
    "        decoded = self.decoder(context)\n",
    "        return self.output_layer(decoded)\n",
    "\n",
    "# 3. Prepare data\n",
    "train_df, val_df = train_test_split(df_clean, test_size=0.2)\n",
    "\n",
    "# Initialize model\n",
    "translator = RobustTranslator()\n",
    "\n",
    "# Adapt the vectorizers - now using .tolist() to ensure proper conversion\n",
    "translator.sv_encoder.adapt(train_df['swedish'].tolist())\n",
    "translator.en_decoder.adapt(train_df['english'].tolist())\n",
    "\n",
    "# 4. Data preparation with proper padding\n",
    "def prepare_data(sv_text, en_text):\n",
    "    # Convert to lists to avoid Tensor conversion issues\n",
    "    sv_text = sv_text.tolist()\n",
    "    en_text = en_text.tolist()\n",
    "    \n",
    "    # Get sequences\n",
    "    sv_seq = translator.sv_encoder(sv_text)\n",
    "    en_seq = translator.en_decoder(en_text)\n",
    "    \n",
    "    # Teacher forcing setup\n",
    "    return (sv_seq, en_seq[:, :-1]), en_seq[:, 1:]\n",
    "\n",
    "# Create datasets\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (train_df['swedish'], train_df['english'])\n",
    ").batch(32).map(prepare_data)\n",
    "\n",
    "val_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (val_df['swedish'], val_df['english'])\n",
    ").batch(32).map(prepare_data)\n",
    "\n",
    "# 5. Compile and train\n",
    "translator.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = translator.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d1ff972",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type NoneType).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5512\\3399050282.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;31m# Adapt the text vectorizers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m \u001b[0mtranslator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msv_encoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madapt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'swedish'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[0mtranslator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0men_decoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madapt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\layers\\preprocessing\\text_vectorization.py\u001b[0m in \u001b[0;36madapt\u001b[1;34m(self, data, batch_size, steps)\u001b[0m\n\u001b[0;32m    421\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 423\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"string\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    424\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m                 \u001b[1;31m# A plain list of strings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\utils\\tf_utils.py\u001b[0m in \u001b[0;36mensure_tensor\u001b[1;34m(inputs, dtype)\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[1;31m# Plain `np.asarray()` conversion fails with PyTorch.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    106\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m   \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type NoneType)."
     ]
    }
   ],
   "source": [
    "import keras_nlp\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class TransformerTranslator(keras.Model):\n",
    "    def __init__(self, max_vocab_size=20000, max_length=50, embed_dim=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Swedish encoder\n",
    "        self.sv_encoder = keras.layers.TextVectorization(\n",
    "            max_tokens=max_vocab_size, output_sequence_length=max_length)\n",
    "        \n",
    "        # English decoder\n",
    "        self.en_decoder = keras.layers.TextVectorization(\n",
    "            max_tokens=max_vocab_size, output_sequence_length=max_length)\n",
    "        \n",
    "        # Embedding layers\n",
    "        self.sv_embedding = keras.layers.Embedding(max_vocab_size, embed_dim)\n",
    "        self.en_embedding = keras.layers.Embedding(max_vocab_size, embed_dim)\n",
    "        \n",
    "        # Transformer\n",
    "        self.transformer = keras_nlp.layers.TransformerEncoder(\n",
    "            num_heads=8,\n",
    "            intermediate_dim=512,\n",
    "            dropout=0.1\n",
    "        )\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = keras.layers.Dense(max_vocab_size, activation='softmax')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        sv_text, en_text = inputs\n",
    "        \n",
    "        # Encode Swedish\n",
    "        sv_tokens = self.sv_encoder(sv_text)\n",
    "        sv_emb = self.sv_embedding(sv_tokens)\n",
    "        encoded = self.transformer(sv_emb)\n",
    "        \n",
    "        # Decode English\n",
    "        en_tokens = self.en_decoder(en_text)\n",
    "        en_emb = self.en_embedding(en_tokens)\n",
    "        \n",
    "        # Simple cross-attention (for demo purposes)\n",
    "        attention = tf.matmul(en_emb, encoded, transpose_b=True)\n",
    "        attention = tf.nn.softmax(attention)\n",
    "        context = tf.matmul(attention, encoded)\n",
    "        \n",
    "        combined = tf.concat([en_emb, context], axis=-1)\n",
    "        return self.output_layer(combined)\n",
    "\n",
    "# Usage\n",
    "train_df, val_df = train_test_split(df, test_size=0.2)\n",
    "translator = TransformerTranslator()\n",
    "\n",
    "# Adapt the text vectorizers\n",
    "translator.sv_encoder.adapt(train_df['swedish'])\n",
    "translator.en_decoder.adapt(train_df['english'])\n",
    "\n",
    "# Compile and train\n",
    "translator.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Prepare data - need to shift decoder inputs for teacher forcing\n",
    "def prepare_data(sv_text, en_text):\n",
    "    # Convert texts to sequences\n",
    "    sv_seq = translator.sv_encoder(sv_text)\n",
    "    en_seq = translator.en_decoder(en_text)\n",
    "    \n",
    "    # For teacher forcing: decoder_inputs is en_seq[:-1], targets are en_seq[1:]\n",
    "    return (sv_seq, en_seq[:, :-1]), en_seq[:, 1:]\n",
    "\n",
    "# Create tf.data.Dataset\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (train_df['swedish'], train_df['english'])\n",
    ").batch(32).map(prepare_data)\n",
    "\n",
    "val_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (val_df['swedish'], val_df['english'])\n",
    ").batch(32).map(prepare_data)\n",
    "\n",
    "# Train\n",
    "translator.fit(train_ds, validation_data=val_ds, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db5eaa8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
